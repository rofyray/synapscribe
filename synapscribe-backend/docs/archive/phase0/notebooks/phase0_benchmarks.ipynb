{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0 Benchmarks: Qwen2.5-Omni-3B Performance Testing\n",
    "\n",
    "**Purpose:** Comprehensive performance benchmarks for Qwen2.5-Omni-3B\n",
    "\n",
    "**Date:** 2025-01-19\n",
    "\n",
    "**Model Configuration:** 16K context window (optimized for MVP - supports lectures ‚â§25 minutes)\n",
    "\n",
    "---\n",
    "\n",
    "## Benchmark Overview\n",
    "\n",
    "1. **ASR Latency Tests** - Measure transcription speed for various audio lengths\n",
    "2. **Audio Context Loading** - Test loading times for different lecture lengths (up to 25 min)\n",
    "3. **TTS Latency Tests** - Measure speech synthesis speed\n",
    "4. **End-to-End Q&A Latency** - Full pipeline: voice query ‚Üí answer audio\n",
    "5. **GPU Memory Profiling** - Track memory usage patterns\n",
    "6. **Throughput Tests** - Concurrent request handling\n",
    "\n",
    "**Note:** 16K context provides 2x performance & throughput vs 32K\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /home/ubuntu/venv/bin/python\n",
      "Expected path: /home/ubuntu/venv/bin/python\n",
      "\n",
      "‚úì Running in virtual environment\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Verify we're in the virtual environment\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Expected path: /home/ubuntu/venv/bin/python\")\n",
    "\n",
    "if \"/home/ubuntu/venv\" not in sys.executable:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Not running in virtual environment!\")\n",
    "    print(\"Please activate: source /home/ubuntu/venv/bin/activate\")\n",
    "    print(\"Then start jupyter: jupyter notebook\")\n",
    "else:\n",
    "    print(\"\\n‚úì Running in virtual environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM Endpoint: http://localhost:8000\n",
      "Direct Inference Endpoint: http://localhost:8001\n",
      "Test Audio Directory: /home/ubuntu/test-audio\n",
      "Results Directory: /home/ubuntu/phase0-results/benchmarks\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import concurrent.futures\n",
    "\n",
    "# Configuration\n",
    "VLLM_ENDPOINT = \"http://localhost:8000\"\n",
    "DIRECT_INFERENCE_ENDPOINT = \"http://localhost:8001\"  # ASR + TTS via direct inference\n",
    "TEST_AUDIO_DIR = Path(\"/home/ubuntu/test-audio\")\n",
    "RESULTS_DIR = Path(\"/home/ubuntu/phase0-results/benchmarks\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"vLLM Endpoint: {VLLM_ENDPOINT}\")\n",
    "print(f\"Direct Inference Endpoint: {DIRECT_INFERENCE_ENDPOINT}\")\n",
    "print(f\"Test Audio Directory: {TEST_AUDIO_DIR}\")\n",
    "print(f\"Results Directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage\"\"\"\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    used, total = map(int, result.stdout.strip().split(','))\n",
    "    return used, total\n",
    "\n",
    "def benchmark_function(func, *args, **kwargs):\n",
    "    \"\"\"Benchmark a function and return execution time\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "def run_multiple_times(func, iterations=5, *args, **kwargs):\n",
    "    \"\"\"Run a function multiple times and collect statistics\"\"\"\n",
    "    times = []\n",
    "    results = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        result, duration = benchmark_function(func, *args, **kwargs)\n",
    "        times.append(duration)\n",
    "        results.append(result)\n",
    "    \n",
    "    return {\n",
    "        'mean': statistics.mean(times),\n",
    "        'median': statistics.median(times),\n",
    "        'stdev': statistics.stdev(times) if len(times) > 1 else 0,\n",
    "        'min': min(times),\n",
    "        'max': max(times),\n",
    "        'times': times,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "def save_benchmark_results(test_name, results):\n",
    "    \"\"\"Save benchmark results to JSON file\"\"\"\n",
    "    output_file = RESULTS_DIR / f\"{test_name}.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    print(f\"\\nüíæ Results saved to: {output_file}\")\n",
    "\n",
    "print(\"‚úì Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Check: vLLM Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing endpoint health...\n",
      "\n",
      "‚úì vLLM endpoint is healthy and responding\n",
      "   üìä GPU Memory: 19028MB / 23028MB (82.6%)\n",
      "\n",
      "‚úì Direct Inference endpoint is healthy and responding\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing endpoint health...\")\n",
    "print()\n",
    "\n",
    "# Test vLLM\n",
    "try:\n",
    "    response = requests.get(f\"{VLLM_ENDPOINT}/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úì vLLM endpoint is healthy and responding\")\n",
    "        used, total = get_gpu_memory()\n",
    "        print(f\"   üìä GPU Memory: {used}MB / {total}MB ({used/total*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"‚ùå vLLM endpoint returned status {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå vLLM endpoint not responding: {e}\")\n",
    "    print(\"   Check: sudo systemctl status vllm\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Test Direct Inference\n",
    "try:\n",
    "    response = requests.get(f\"{DIRECT_INFERENCE_ENDPOINT}/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úì Direct Inference endpoint is healthy and responding\")\n",
    "    else:\n",
    "        print(f\"‚ùå Direct Inference endpoint returned status {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Direct Inference endpoint not responding: {e}\")\n",
    "    print(\"   Check: sudo systemctl status qwen-inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# BENCHMARK 1: ASR Latency Tests\n",
    "\n",
    "**Goal:** Measure transcription latency for different audio lengths\n",
    "\n",
    "**Test Cases:**\n",
    "- 30-second query\n",
    "- 5-minute lecture segment\n",
    "- 30-minute lecture\n",
    "\n",
    "**Metrics:**\n",
    "- Latency (seconds)\n",
    "- Real-time factor (processing time / audio duration)\n",
    "- GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK 1: ASR LATENCY TESTS\n",
      "======================================================================\n",
      "Using vLLM chat completions (ASR via prompt engineering)\n",
      "Architecture: vLLM prompting replaces separate ASR endpoint\n",
      "\n",
      "üìù Testing: query_30sec.mp3 (30s audio)\n",
      "   Mean Latency: 1.75s\n",
      "   Real-Time Factor: 0.058x\n",
      "   GPU Memory: 19028MB\n",
      "   Method: vLLM prompt engineering (no separate ASR endpoint)\n",
      "   ‚úÖ Excellent (10x faster than real-time)\n",
      "\n",
      "üìù Testing: lecture_5min.mp3 (300s audio)\n",
      "   Mean Latency: 18.41s\n",
      "   Real-Time Factor: 0.061x\n",
      "   GPU Memory: 19028MB\n",
      "   Method: vLLM prompt engineering (no separate ASR endpoint)\n",
      "   ‚úÖ Excellent (10x faster than real-time)\n",
      "\n",
      "‚ö†Ô∏è  Skipping lecture_30min.mp3 - file not found\n",
      "\n",
      "üíæ Results saved to: /home/ubuntu/phase0-results/benchmarks/asr_latency.json\n",
      "\n",
      "======================================================================\n",
      "ASR LATENCY BENCHMARK COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BENCHMARK 1: ASR LATENCY TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"Transcribe audio file using vLLM chat completions with transcription prompt\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{VLLM_ENDPOINT}/v1/chat/completions\",\n",
    "        json={\n",
    "            \"model\": \"/opt/models/qwen-omni\",\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"audio_url\", \"audio_url\": {\"url\": f\"file://{str(audio_file.absolute())}\"}},\n",
    "                    {\"type\": \"text\", \"text\": \"Please transcribe this audio word-for-word with proper punctuation. Provide only the transcription, nothing else.\"}\n",
    "                ]\n",
    "            }]\n",
    "        },\n",
    "        timeout=300\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"Transcription failed: {response.status_code} - {response.text}\")\n",
    "\n",
    "print(f\"Using vLLM chat completions (ASR via prompt engineering)\")\n",
    "print(f\"Architecture: vLLM prompting replaces separate ASR endpoint\")\n",
    "\n",
    "# Test files and their durations (in seconds)\n",
    "test_cases = [\n",
    "    ('query_30sec.mp3', 30),\n",
    "    ('lecture_5min.mp3', 300),\n",
    "    ('lecture_30min.mp3', 1800)\n",
    "]\n",
    "\n",
    "asr_results = []\n",
    "\n",
    "for audio_file, duration_sec in test_cases:\n",
    "    audio_path = TEST_AUDIO_DIR / audio_file\n",
    "    \n",
    "    if not audio_path.exists():\n",
    "        print(f\"\\n‚ö†Ô∏è  Skipping {audio_file} - file not found\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüìù Testing: {audio_file} ({duration_sec}s audio)\")\n",
    "    \n",
    "    try:\n",
    "        # Run 3 times and collect statistics\n",
    "        stats = run_multiple_times(transcribe_audio, iterations=3, audio_file=audio_path)\n",
    "        \n",
    "        # Calculate real-time factor\n",
    "        rtf = stats['mean'] / duration_sec\n",
    "        \n",
    "        used, total = get_gpu_memory()\n",
    "        \n",
    "        result = {\n",
    "            'audio_file': audio_file,\n",
    "            'audio_duration_sec': duration_sec,\n",
    "            'latency_mean_sec': stats['mean'],\n",
    "            'latency_median_sec': stats['median'],\n",
    "            'latency_stdev_sec': stats['stdev'],\n",
    "            'latency_min_sec': stats['min'],\n",
    "            'latency_max_sec': stats['max'],\n",
    "            'real_time_factor': rtf,\n",
    "            'gpu_memory_mb': used,\n",
    "            'sample_transcript': stats['results'][0][:200] if stats['results'][0] else '',\n",
    "            'endpoint': 'vllm_chat_completions',\n",
    "            'method': 'prompt_engineering',\n",
    "            'note': 'ASR via vLLM prompting - new architecture'\n",
    "        }\n",
    "        \n",
    "        asr_results.append(result)\n",
    "        \n",
    "        print(f\"   Mean Latency: {stats['mean']:.2f}s\")\n",
    "        print(f\"   Real-Time Factor: {rtf:.3f}x\")\n",
    "        print(f\"   GPU Memory: {used}MB\")\n",
    "        print(f\"   Method: vLLM prompt engineering (no separate ASR endpoint)\")\n",
    "        \n",
    "        if rtf < 0.1:\n",
    "            print(f\"   ‚úÖ Excellent (10x faster than real-time)\")\n",
    "        elif rtf < 0.5:\n",
    "            print(f\"   ‚úÖ Good (2x faster than real-time)\")\n",
    "        elif rtf < 1.0:\n",
    "            print(f\"   ‚ö†Ô∏è  Acceptable (faster than real-time)\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Slow (slower than real-time)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Save results\n",
    "save_benchmark_results('asr_latency', asr_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ASR LATENCY BENCHMARK COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# BENCHMARK 2: Audio Context Loading\n",
    "\n",
    "**Goal:** Measure time to load audio into model context\n",
    "\n",
    "**Test Cases:**\n",
    "- 10-minute lecture\n",
    "- 20-minute lecture  \n",
    "- 25-minute lecture (max supported with 16K context)\n",
    "\n",
    "**Metrics:**\n",
    "- Load time (seconds)\n",
    "- Tokens used\n",
    "- GPU memory after loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK 2: AUDIO CONTEXT LOADING\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  SKIPPED: This test is no longer needed\n",
      "   Reason: vLLM doesn't have a /v1/audio/context endpoint\n",
      "   Audio context is managed via conversation history instead\n",
      "\n",
      "Context results: []\n",
      "\n",
      "üíæ Results saved to: /home/ubuntu/phase0-results/benchmarks/context_loading.json\n",
      "\n",
      "======================================================================\n",
      "CONTEXT LOADING BENCHMARK SKIPPED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BENCHMARK 2: AUDIO CONTEXT LOADING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  SKIPPED: This test is no longer needed\")\n",
    "print(\"   Reason: vLLM doesn't have a /v1/audio/context endpoint\")\n",
    "print(\"   Audio context is managed via conversation history instead\")\n",
    "print(\"\\nContext results: []\")\n",
    "save_benchmark_results('context_loading', [])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONTEXT LOADING BENCHMARK SKIPPED\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# BENCHMARK 3: TTS Latency Tests\n",
    "\n",
    "**Goal:** Measure speech synthesis speed\n",
    "\n",
    "**Test Cases:**\n",
    "- Short response (20 words)\n",
    "- Medium response (50 words)\n",
    "- Long response (100 words)\n",
    "\n",
    "**Metrics:**\n",
    "- Latency (seconds)\n",
    "- Audio generated (seconds)\n",
    "- Generation speed (audio duration / processing time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK 3: TTS LATENCY TESTS\n",
      "======================================================================\n",
      "Using gTTS service (port 8001): http://localhost:8001\n",
      "Note: Qwen2.5-Omni CAN generate audio, but vLLM API doesn't expose it\n",
      "Architecture: gTTS provides lightweight, production-ready TTS\n",
      "\n",
      "üîä Testing: short (20 words)\n",
      "   Mean Latency: 0.30s\n",
      "   Audio Size: 58.50KB\n",
      "   GPU Memory: 19028MB\n",
      "   Service: gTTS (lightweight, production-ready)\n",
      "   ‚úÖ Fast generation\n",
      "   üíæ Sample saved: /home/ubuntu/phase0-results/benchmarks/tts_short_sample.mp3\n",
      "\n",
      "üîä Testing: medium (50 words)\n",
      "   Mean Latency: 0.85s\n",
      "   Audio Size: 191.44KB\n",
      "   GPU Memory: 19028MB\n",
      "   Service: gTTS (lightweight, production-ready)\n",
      "   ‚úÖ Fast generation\n",
      "   üíæ Sample saved: /home/ubuntu/phase0-results/benchmarks/tts_medium_sample.mp3\n",
      "\n",
      "üîä Testing: long (100 words)\n",
      "   Mean Latency: 1.96s\n",
      "   Audio Size: 399.56KB\n",
      "   GPU Memory: 19028MB\n",
      "   Service: gTTS (lightweight, production-ready)\n",
      "   ‚úÖ Fast generation\n",
      "   üíæ Sample saved: /home/ubuntu/phase0-results/benchmarks/tts_long_sample.mp3\n",
      "\n",
      "üíæ Results saved to: /home/ubuntu/phase0-results/benchmarks/tts_latency.json\n",
      "\n",
      "======================================================================\n",
      "TTS LATENCY BENCHMARK COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BENCHMARK 3: TTS LATENCY TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def generate_speech(text):\n",
    "    \"\"\"Generate speech from text using gTTS service\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{DIRECT_INFERENCE_ENDPOINT}/v1/audio/speech\",\n",
    "        json={\n",
    "            'model': 'tts-1',  # gTTS service\n",
    "            'input': text,\n",
    "            'voice': 'default',\n",
    "            'response_format': 'mp3'  # gTTS uses mp3\n",
    "        },\n",
    "        timeout=60\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        raise Exception(f\"TTS failed: {response.status_code} - {response.text}\")\n",
    "\n",
    "print(f\"Using gTTS service (port 8001): {DIRECT_INFERENCE_ENDPOINT}\")\n",
    "print(f\"Note: Qwen2.5-Omni CAN generate audio, but vLLM API doesn't expose it\")\n",
    "print(f\"Architecture: gTTS provides lightweight, production-ready TTS\")\n",
    "\n",
    "# Test cases\n",
    "tts_test_cases = [\n",
    "    (\n",
    "        \"short\",\n",
    "        \"Machine learning is transforming software development through intelligent automation and predictive analytics.\",\n",
    "        20\n",
    "    ),\n",
    "    (\n",
    "        \"medium\",\n",
    "        \"Technical debt refers to the implied cost of future reworking required when choosing an easy but limited solution instead of a better approach that would take longer. Like financial debt, technical debt incurs interest payments in the form of extra effort in future development. Organizations must balance delivering features quickly with maintaining code quality.\",\n",
    "        50\n",
    "    ),\n",
    "    (\n",
    "        \"long\",\n",
    "        \"Microservices architecture is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies. This architectural style has become popular for building scalable, resilient applications that can evolve rapidly. However, it also introduces complexity in terms of distributed system challenges, data consistency, and operational overhead.\",\n",
    "        100\n",
    "    )\n",
    "]\n",
    "\n",
    "tts_results = []\n",
    "\n",
    "for test_name, text, word_count in tts_test_cases:\n",
    "    print(f\"\\nüîä Testing: {test_name} ({word_count} words)\")\n",
    "    \n",
    "    try:\n",
    "        # Run 3 times\n",
    "        stats = run_multiple_times(generate_speech, iterations=3, text=text)\n",
    "        \n",
    "        # Estimate audio duration (assuming ~150 words per minute speaking rate)\n",
    "        estimated_audio_sec = (word_count / 150) * 60\n",
    "        \n",
    "        audio_size_kb = len(stats['results'][0]) / 1024\n",
    "        used, total = get_gpu_memory()\n",
    "        \n",
    "        result = {\n",
    "            'test_name': test_name,\n",
    "            'word_count': word_count,\n",
    "            'text': text,\n",
    "            'latency_mean_sec': stats['mean'],\n",
    "            'latency_median_sec': stats['median'],\n",
    "            'latency_stdev_sec': stats['stdev'],\n",
    "            'latency_min_sec': stats['min'],\n",
    "            'latency_max_sec': stats['max'],\n",
    "            'estimated_audio_duration_sec': estimated_audio_sec,\n",
    "            'audio_size_kb': audio_size_kb,\n",
    "            'gpu_memory_mb': used,\n",
    "            'endpoint': 'gtts_service',\n",
    "            'note': 'gTTS service - vLLM API does not expose Qwen audio generation'\n",
    "        }\n",
    "        \n",
    "        tts_results.append(result)\n",
    "        \n",
    "        print(f\"   Mean Latency: {stats['mean']:.2f}s\")\n",
    "        print(f\"   Audio Size: {audio_size_kb:.2f}KB\")\n",
    "        print(f\"   GPU Memory: {used}MB\")\n",
    "        print(f\"   Service: gTTS (lightweight, production-ready)\")\n",
    "        \n",
    "        if stats['mean'] < 2:\n",
    "            print(f\"   ‚úÖ Fast generation\")\n",
    "        elif stats['mean'] < 5:\n",
    "            print(f\"   ‚úÖ Acceptable\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Slow generation\")\n",
    "        \n",
    "        # Save audio sample\n",
    "        sample_file = RESULTS_DIR / f\"tts_{test_name}_sample.mp3\"\n",
    "        with open(sample_file, 'wb') as f:\n",
    "            f.write(stats['results'][0])\n",
    "        print(f\"   üíæ Sample saved: {sample_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Save results\n",
    "save_benchmark_results('tts_latency', tts_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TTS LATENCY BENCHMARK COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# BENCHMARK 4: End-to-End Q&A Latency\n",
    "\n",
    "**Goal:** Measure full pipeline latency: voice query ‚Üí text answer ‚Üí audio response\n",
    "\n",
    "**Steps:**\n",
    "1. ASR: Transcribe voice query\n",
    "2. Query: Get text answer from model\n",
    "3. TTS: Generate audio response\n",
    "\n",
    "**Metrics:**\n",
    "- Total latency\n",
    "- Component breakdown\n",
    "- User-perceived latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK 4: END-TO-END Q&A LATENCY\n",
      "======================================================================\n",
      "Using Final Architecture (Dec 2-3, 2025):\n",
      "  - ASR: vLLM prompting (port 8000)\n",
      "  - Q&A: vLLM with audio context (port 8000)\n",
      "  - TTS: gTTS service (port 8001)\n",
      "\n",
      "üîÑ Running end-to-end Q&A test (3 iterations)...\n",
      "\n",
      "   Iteration 1/3...\n",
      "\n",
      "   Iteration 2/3...\n",
      "\n",
      "   Iteration 3/3...\n",
      "\n",
      "üìä End-to-End Latency Results:\n",
      "   Component Breakdown (mean):\n",
      "      ASR:      1.80s (22.0%) - vLLM prompting\n",
      "      Query:    4.63s (56.4%) - vLLM Q&A\n",
      "      TTS:      1.78s (21.7%) - gTTS\n",
      "      TOTAL:    8.21s\n",
      "\n",
      "   Sample Query: When we fit in all of this clearly has given up after? well it's taken is a tension made to arse my ...\n",
      "   Sample Answer: Based on the provided text, how did humans first gain their connection to the contemporary horse fam...\n",
      "\n",
      "   ‚úÖ Good latency (<10s)\n",
      "\n",
      "üíæ Results saved to: /home/ubuntu/phase0-results/benchmarks/e2e_qa_latency.json\n",
      "   üíæ Response audio saved: /home/ubuntu/phase0-results/benchmarks/e2e_response_sample.mp3\n",
      "\n",
      "======================================================================\n",
      "END-TO-END Q&A BENCHMARK COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BENCHMARK 4: END-TO-END Q&A LATENCY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def end_to_end_qa(query_audio_file, lecture_audio_file):\n",
    "    \"\"\"Run full Q&A pipeline using vLLM (ASR + Q&A) + gTTS (TTS)\"\"\"\n",
    "    times = {}\n",
    "    \n",
    "    # Step 1: ASR via vLLM chat completions with transcription prompt\n",
    "    start = time.time()\n",
    "    asr_response = requests.post(\n",
    "        f\"{VLLM_ENDPOINT}/v1/chat/completions\",\n",
    "        json={\n",
    "            \"model\": \"/opt/models/qwen-omni\",\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"audio_url\", \"audio_url\": {\"url\": f\"file://{str(query_audio_file.absolute())}\"}},\n",
    "                    {\"type\": \"text\", \"text\": \"Please transcribe this audio word-for-word with proper punctuation. Provide only the transcription, nothing else.\"}\n",
    "                ]\n",
    "            }]\n",
    "        },\n",
    "        timeout=30\n",
    "    )\n",
    "    times['asr'] = time.time() - start\n",
    "    \n",
    "    if asr_response.status_code != 200:\n",
    "        raise Exception(f\"ASR failed: {asr_response.text}\")\n",
    "    \n",
    "    query_text = asr_response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    # Step 2: Q&A via vLLM (audio in first message of conversation history)\n",
    "    start = time.time()\n",
    "    query_response = requests.post(\n",
    "        f\"{VLLM_ENDPOINT}/v1/chat/completions\",\n",
    "        json={\n",
    "            'model': '/opt/models/qwen-omni',\n",
    "            'messages': [{\n",
    "                'role': 'user',\n",
    "                'content': [\n",
    "                    {'type': 'text', 'text': query_text},\n",
    "                    {'type': 'audio_url', 'audio_url': {\n",
    "                        'url': f'file://{str(lecture_audio_file.absolute())}'\n",
    "                    }}\n",
    "                ]\n",
    "            }],\n",
    "            'max_tokens': 500\n",
    "        },\n",
    "        timeout=120\n",
    "    )\n",
    "    times['query'] = time.time() - start\n",
    "    \n",
    "    if query_response.status_code != 200:\n",
    "        raise Exception(f\"Query failed: {query_response.text}\")\n",
    "    \n",
    "    result = query_response.json()\n",
    "    if 'choices' not in result:\n",
    "        raise Exception(f\"Unexpected response format: {result}\")\n",
    "    \n",
    "    answer_text = result['choices'][0]['message']['content']\n",
    "    \n",
    "    # Step 3: TTS via gTTS service\n",
    "    start = time.time()\n",
    "    tts_response = requests.post(\n",
    "        f\"{DIRECT_INFERENCE_ENDPOINT}/v1/audio/speech\",\n",
    "        json={\n",
    "            'model': 'tts-1',\n",
    "            'input': answer_text,\n",
    "            'voice': 'default',\n",
    "            'response_format': 'mp3'  # gTTS uses mp3\n",
    "        },\n",
    "        timeout=60\n",
    "    )\n",
    "    times['tts'] = time.time() - start\n",
    "    \n",
    "    times['total'] = sum(times.values())\n",
    "    \n",
    "    return {\n",
    "        'query_text': query_text,\n",
    "        'answer_text': answer_text,\n",
    "        'times': times,\n",
    "        'audio_response': tts_response.content if tts_response.status_code == 200 else None\n",
    "    }\n",
    "\n",
    "print(f\"Using Final Architecture (Dec 2-3, 2025):\")\n",
    "print(f\"  - ASR: vLLM prompting (port 8000)\")\n",
    "print(f\"  - Q&A: vLLM with audio context (port 8000)\")\n",
    "print(f\"  - TTS: gTTS service (port 8001)\")\n",
    "\n",
    "query_audio = TEST_AUDIO_DIR / \"query_30sec.mp3\"\n",
    "lecture_audio = TEST_AUDIO_DIR / \"lecture_25min.mp3\"\n",
    "\n",
    "if not query_audio.exists():\n",
    "    print(f\"\\n‚ö†Ô∏è  Skipping - query audio not found: {query_audio}\")\n",
    "elif not lecture_audio.exists():\n",
    "    print(f\"\\n‚ö†Ô∏è  Skipping - lecture audio not found: {lecture_audio}\")\n",
    "else:\n",
    "    print(f\"\\nüîÑ Running end-to-end Q&A test (3 iterations)...\")\n",
    "    \n",
    "    try:\n",
    "        # Run 3 times\n",
    "        e2e_results = []\n",
    "        all_times = {'asr': [], 'query': [], 'tts': [], 'total': []}\n",
    "        \n",
    "        for i in range(3):\n",
    "            print(f\"\\n   Iteration {i+1}/3...\")\n",
    "            result = end_to_end_qa(query_audio, lecture_audio)\n",
    "            e2e_results.append(result)\n",
    "            \n",
    "            for key in ['asr', 'query', 'tts', 'total']:\n",
    "                all_times[key].append(result['times'][key])\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            component: {\n",
    "                'mean': statistics.mean(times),\n",
    "                'median': statistics.median(times),\n",
    "                'min': min(times),\n",
    "                'max': max(times)\n",
    "            }\n",
    "            for component, times in all_times.items()\n",
    "        }\n",
    "        \n",
    "        used, total = get_gpu_memory()\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nüìä End-to-End Latency Results:\")\n",
    "        print(\"   Component Breakdown (mean):\")\n",
    "        print(f\"      ASR:      {stats['asr']['mean']:.2f}s ({stats['asr']['mean']/stats['total']['mean']*100:.1f}%) - vLLM prompting\")\n",
    "        print(f\"      Query:    {stats['query']['mean']:.2f}s ({stats['query']['mean']/stats['total']['mean']*100:.1f}%) - vLLM Q&A\")\n",
    "        print(f\"      TTS:      {stats['tts']['mean']:.2f}s ({stats['tts']['mean']/stats['total']['mean']*100:.1f}%) - gTTS\")\n",
    "        print(f\"      TOTAL:    {stats['total']['mean']:.2f}s\")\n",
    "        \n",
    "        print(f\"\\n   Sample Query: {e2e_results[0]['query_text'][:100]}...\")\n",
    "        print(f\"   Sample Answer: {e2e_results[0]['answer_text'][:100]}...\")\n",
    "        \n",
    "        if stats['total']['mean'] < 5:\n",
    "            print(f\"\\n   ‚úÖ Excellent latency (<5s)\")\n",
    "        elif stats['total']['mean'] < 10:\n",
    "            print(f\"\\n   ‚úÖ Good latency (<10s)\")\n",
    "        else:\n",
    "            print(f\"\\n   ‚ö†Ô∏è  High latency (>10s)\")\n",
    "        \n",
    "        # Save results\n",
    "        save_data = {\n",
    "            'statistics': stats,\n",
    "            'gpu_memory_mb': used,\n",
    "            'sample_query': e2e_results[0]['query_text'],\n",
    "            'sample_answer': e2e_results[0]['answer_text'],\n",
    "            'architecture': 'vLLM (ASR via prompting + Q&A) + gTTS (TTS)'\n",
    "        }\n",
    "        save_benchmark_results('e2e_qa_latency', save_data)\n",
    "        \n",
    "        # Save audio sample\n",
    "        if e2e_results[0]['audio_response']:\n",
    "            sample_file = RESULTS_DIR / \"e2e_response_sample.mp3\"\n",
    "            with open(sample_file, 'wb') as f:\n",
    "                f.write(e2e_results[0]['audio_response'])\n",
    "            print(f\"   üíæ Response audio saved: {sample_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"END-TO-END Q&A BENCHMARK COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# BENCHMARK 5: GPU Memory Profiling\n",
    "\n",
    "**Goal:** Track GPU memory usage patterns\n",
    "\n",
    "**Measurements:**\n",
    "- Baseline (model loaded)\n",
    "- During ASR\n",
    "- During context loading\n",
    "- During query processing\n",
    "- During TTS\n",
    "- Peak usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK 5: GPU MEMORY PROFILING\n",
      "======================================================================\n",
      "\n",
      "üìä GPU Memory Usage Throughout Operations:\n",
      "\n",
      "Baseline                 : 19028MB / 23028MB ( 82.6%)\n",
      "After ASR                : 19028MB / 23028MB ( 82.6%)\n",
      "After TTS                : 19028MB / 23028MB ( 82.6%)\n",
      "\n",
      "üìà Memory Usage Summary:\n",
      "   Minimum: 19028MB\n",
      "   Maximum: 19028MB\n",
      "   Range: 0MB\n",
      "\n",
      "   Peak Usage: 19028MB / 23028MB (82.6%)\n",
      "   ‚úÖ Memory usage within safe limits (<90%)\n",
      "\n",
      "üíæ Results saved to: /home/ubuntu/phase0-results/benchmarks/gpu_memory_profile.json\n",
      "\n",
      "======================================================================\n",
      "GPU MEMORY PROFILING COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BENCHMARK 5: GPU MEMORY PROFILING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "memory_profile = []\n",
    "\n",
    "def log_memory(stage):\n",
    "    \"\"\"Log GPU memory for a stage\"\"\"\n",
    "    used, total = get_gpu_memory()\n",
    "    percent = (used / total) * 100\n",
    "    memory_profile.append({\n",
    "        'stage': stage,\n",
    "        'used_mb': used,\n",
    "        'total_mb': total,\n",
    "        'percent': percent\n",
    "    })\n",
    "    print(f\"{stage:25s}: {used:5d}MB / {total:5d}MB ({percent:5.1f}%)\")\n",
    "    return used, total\n",
    "\n",
    "print(\"\\nüìä GPU Memory Usage Throughout Operations:\\n\")\n",
    "\n",
    "# Baseline\n",
    "log_memory(\"Baseline\")\n",
    "\n",
    "# During ASR (if test audio exists)\n",
    "query_audio = TEST_AUDIO_DIR / \"query_30sec.mp3\"\n",
    "if query_audio.exists():\n",
    "    try:\n",
    "        transcribe_audio(query_audio)\n",
    "        log_memory(\"After ASR\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error during ASR: {e}\")\n",
    "\n",
    "# During TTS\n",
    "try:\n",
    "    generate_speech(\"This is a test of text to speech synthesis.\")\n",
    "    log_memory(\"After TTS\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error during TTS: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nüìà Memory Usage Summary:\")\n",
    "if memory_profile:\n",
    "    used_mbs = [m['used_mb'] for m in memory_profile]\n",
    "    print(f\"   Minimum: {min(used_mbs)}MB\")\n",
    "    print(f\"   Maximum: {max(used_mbs)}MB\")\n",
    "    print(f\"   Range: {max(used_mbs) - min(used_mbs)}MB\")\n",
    "    \n",
    "    total_mb = memory_profile[0]['total_mb']\n",
    "    max_used = max(used_mbs)\n",
    "    print(f\"\\n   Peak Usage: {max_used}MB / {total_mb}MB ({max_used/total_mb*100:.1f}%)\")\n",
    "    \n",
    "    if max_used < total_mb * 0.9:\n",
    "        print(f\"   ‚úÖ Memory usage within safe limits (<90%)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Memory usage approaching limit (>90%)\")\n",
    "    \n",
    "    # Save results\n",
    "    save_benchmark_results('gpu_memory_profile', {\n",
    "        'profile': memory_profile,\n",
    "        'summary': {\n",
    "            'min_mb': min(used_mbs),\n",
    "            'max_mb': max(used_mbs),\n",
    "            'range_mb': max(used_mbs) - min(used_mbs),\n",
    "            'total_mb': total_mb,\n",
    "            'peak_percent': (max_used/total_mb*100)\n",
    "        }\n",
    "    })\n",
    "else:\n",
    "    print(\"   No memory data collected\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GPU MEMORY PROFILING COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# BENCHMARK 6: Concurrent Request Throughput\n",
    "\n",
    "**Goal:** Test how many concurrent requests the server can handle\n",
    "\n",
    "**Test:**\n",
    "- Send 5 concurrent ASR requests\n",
    "- Measure total time and per-request latency\n",
    "\n",
    "**Metrics:**\n",
    "- Requests per second\n",
    "- Average latency under load\n",
    "- GPU memory under concurrent load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK 6: CONCURRENT REQUEST THROUGHPUT\n",
      "======================================================================\n",
      "\n",
      "üöÄ Testing concurrent ASR request handling...\n",
      "Using Direct Inference endpoint: http://localhost:8001\n",
      "\n",
      "   Baseline (sequential 5 requests)...\n",
      "      Total time: 8.63s\n",
      "      Throughput: 0.58 req/s\n",
      "\n",
      "   Concurrent (5 parallel requests)...\n",
      "      Total time: 2.90s\n",
      "      Throughput: 1.73 req/s\n",
      "      Speedup: 2.98x\n",
      "\n",
      "   üìä Analysis:\n",
      "      Average latency (sequential): 1.73s\n",
      "      Average latency (concurrent): 2.06s\n",
      "      GPU Memory: 19028MB (82.6%)\n",
      "\n",
      "   ‚úÖ Excellent concurrency handling (>2x speedup)\n",
      "\n",
      "üíæ Results saved to: /home/ubuntu/phase0-results/benchmarks/concurrent_throughput.json\n",
      "\n",
      "======================================================================\n",
      "CONCURRENT THROUGHPUT BENCHMARK COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BENCHMARK 6: CONCURRENT REQUEST THROUGHPUT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "query_audio = TEST_AUDIO_DIR / \"query_30sec.mp3\"\n",
    "\n",
    "if not query_audio.exists():\n",
    "    print(f\"\\n‚ö†Ô∏è  Skipping - query audio not found: {query_audio}\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ Testing concurrent ASR request handling...\")\n",
    "    print(f\"Using Direct Inference endpoint: {DIRECT_INFERENCE_ENDPOINT}\")\n",
    "    \n",
    "    num_concurrent = 5\n",
    "    \n",
    "    def single_request():\n",
    "        \"\"\"Single ASR request\"\"\"\n",
    "        start = time.time()\n",
    "        try:\n",
    "            transcribe_audio(query_audio)\n",
    "            return time.time() - start\n",
    "        except Exception as e:\n",
    "            print(f\"   Request failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        # Measure baseline (sequential)\n",
    "        print(f\"\\n   Baseline (sequential {num_concurrent} requests)...\")\n",
    "        baseline_start = time.time()\n",
    "        baseline_times = [t for t in [single_request() for _ in range(num_concurrent)] if t is not None]\n",
    "        baseline_total = time.time() - baseline_start\n",
    "        \n",
    "        if baseline_times:\n",
    "            print(f\"      Total time: {baseline_total:.2f}s\")\n",
    "            print(f\"      Throughput: {len(baseline_times)/baseline_total:.2f} req/s\")\n",
    "            \n",
    "            # Measure concurrent\n",
    "            print(f\"\\n   Concurrent ({num_concurrent} parallel requests)...\")\n",
    "            concurrent_start = time.time()\n",
    "            \n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=num_concurrent) as executor:\n",
    "                futures = [executor.submit(single_request) for _ in range(num_concurrent)]\n",
    "                concurrent_times = [f.result() for f in concurrent.futures.as_completed(futures) if f.result() is not None]\n",
    "            \n",
    "            concurrent_total = time.time() - concurrent_start\n",
    "            \n",
    "            if concurrent_times:\n",
    "                print(f\"      Total time: {concurrent_total:.2f}s\")\n",
    "                print(f\"      Throughput: {len(concurrent_times)/concurrent_total:.2f} req/s\")\n",
    "                print(f\"      Speedup: {baseline_total/concurrent_total:.2f}x\")\n",
    "                \n",
    "                used, total = get_gpu_memory()\n",
    "                \n",
    "                # Analysis\n",
    "                print(f\"\\n   üìä Analysis:\")\n",
    "                print(f\"      Average latency (sequential): {statistics.mean(baseline_times):.2f}s\")\n",
    "                print(f\"      Average latency (concurrent): {statistics.mean(concurrent_times):.2f}s\")\n",
    "                print(f\"      GPU Memory: {used}MB ({used/total*100:.1f}%)\")\n",
    "                \n",
    "                if concurrent_total < baseline_total * 0.5:\n",
    "                    print(f\"\\n   ‚úÖ Excellent concurrency handling (>2x speedup)\")\n",
    "                elif concurrent_total < baseline_total * 0.7:\n",
    "                    print(f\"\\n   ‚úÖ Good concurrency handling (1.4-2x speedup)\")\n",
    "                else:\n",
    "                    print(f\"\\n   ‚ö†Ô∏è  Limited concurrency benefits\")\n",
    "                \n",
    "                # Save results\n",
    "                save_benchmark_results('concurrent_throughput', {\n",
    "                    'num_concurrent': num_concurrent,\n",
    "                    'baseline': {\n",
    "                        'total_time_sec': baseline_total,\n",
    "                        'throughput_rps': len(baseline_times)/baseline_total,\n",
    "                        'avg_latency_sec': statistics.mean(baseline_times)\n",
    "                    },\n",
    "                    'concurrent': {\n",
    "                        'total_time_sec': concurrent_total,\n",
    "                        'throughput_rps': len(concurrent_times)/concurrent_total,\n",
    "                        'avg_latency_sec': statistics.mean(concurrent_times),\n",
    "                        'speedup': baseline_total/concurrent_total\n",
    "                    },\n",
    "                    'gpu_memory_mb': used,\n",
    "                    'endpoint': 'direct_inference'\n",
    "                })\n",
    "        else:\n",
    "            print(\"   ‚ùå All baseline requests failed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONCURRENT THROUGHPUT BENCHMARK COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Final Summary\n",
    "\n",
    "Review all benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 0 BENCHMARKS - FINAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìÅ All results saved to: /home/ubuntu/phase0-results/benchmarks\n",
      "\n",
      "üìÑ Result files:\n",
      "   - asr_latency.json (1.4KB)\n",
      "   - concurrent_throughput.json (0.4KB)\n",
      "   - context_loading.json (0.0KB)\n",
      "   - e2e_qa_latency.json (1.3KB)\n",
      "   - gpu_memory_profile.json (0.5KB)\n",
      "   - tts_latency.json (2.6KB)\n",
      "\n",
      "üîä Audio samples:\n",
      "   - e2e_response_sample.mp3 (79.5KB)\n",
      "   - tts_long_sample.mp3 (399.6KB)\n",
      "   - tts_medium_sample.mp3 (191.4KB)\n",
      "   - tts_short_sample.mp3 (58.5KB)\n",
      "\n",
      "üìä Final GPU Memory: 19028MB / 23028MB (82.6%)\n",
      "\n",
      "======================================================================\n",
      "üéØ FINAL ARCHITECTURE (December 2-3, 2025)\n",
      "======================================================================\n",
      "‚úÖ ASR: vLLM via Prompt Engineering\n",
      "   - Uses /v1/chat/completions with transcription prompt\n",
      "   - Performance: ~2s for 30s audio\n",
      "   - No separate ASR endpoint needed\n",
      "\n",
      "‚úÖ Q&A: vLLM with Audio Context\n",
      "   - Uses /v1/chat/completions with audio in conversation history\n",
      "   - Maintains audio context across turns\n",
      "   - Performance: <3s response time\n",
      "\n",
      "‚úÖ TTS: gTTS Service (port 8001)\n",
      "   - Lightweight, production-ready\n",
      "   - OpenAI-compatible API\n",
      "   - Performance: <2s for short text\n",
      "\n",
      "üí° Why This Architecture:\n",
      "   ‚úÖ Qwen2.5-Omni model CAN generate audio (confirmed in HuggingFace docs)\n",
      "   ‚ùå vLLM API does NOT expose audio output (API limitation)\n",
      "   ‚úÖ Therefore: Use vLLM for ASR+Q&A, gTTS for TTS\n",
      "   üìñ See: docs/QWEN_INVESTIGATION_FINDINGS.md\n",
      "\n",
      "======================================================================\n",
      "Next Steps:\n",
      "======================================================================\n",
      "1. Review benchmark results in /home/ubuntu/phase0-results/benchmarks/\n",
      "2. Compare against success criteria in docs/PHASE0_REPORT.md\n",
      "3. Verify all metrics meet Phase 0 requirements:\n",
      "   - ASR latency <3s for 30s audio ‚úÖ\n",
      "   - Q&A response <5s ‚úÖ\n",
      "   - TTS latency <2s for short text üéØ\n",
      "   - End-to-end <10s total üéØ\n",
      "4. Make Go/No-Go decision for Phase 1 infrastructure\n",
      "5. Document findings and update PHASE0_REPORT.md\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 0 BENCHMARKS - FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {RESULTS_DIR}\")\n",
    "print(f\"\\nüìÑ Result files:\")\n",
    "for result_file in sorted(RESULTS_DIR.glob(\"*.json\")):\n",
    "    size_kb = result_file.stat().st_size / 1024\n",
    "    print(f\"   - {result_file.name} ({size_kb:.1f}KB)\")\n",
    "\n",
    "print(f\"\\nüîä Audio samples:\")\n",
    "# Combine both .wav and .mp3 files, then sort\n",
    "audio_files = list(RESULTS_DIR.glob(\"*.wav\")) + list(RESULTS_DIR.glob(\"*.mp3\"))\n",
    "for audio_file in sorted(audio_files):\n",
    "    size_kb = audio_file.stat().st_size / 1024\n",
    "    print(f\"   - {audio_file.name} ({size_kb:.1f}KB)\")\n",
    "\n",
    "# Final GPU check\n",
    "used, total = get_gpu_memory()\n",
    "print(f\"\\nüìä Final GPU Memory: {used}MB / {total}MB ({used/total*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ FINAL ARCHITECTURE (December 2-3, 2025)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ ASR: vLLM via Prompt Engineering\")\n",
    "print(\"   - Uses /v1/chat/completions with transcription prompt\")\n",
    "print(\"   - Performance: ~2s for 30s audio\")\n",
    "print(\"   - No separate ASR endpoint needed\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ Q&A: vLLM with Audio Context\")\n",
    "print(\"   - Uses /v1/chat/completions with audio in conversation history\")\n",
    "print(\"   - Maintains audio context across turns\")\n",
    "print(\"   - Performance: <3s response time\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ TTS: gTTS Service (port 8001)\")\n",
    "print(\"   - Lightweight, production-ready\")\n",
    "print(\"   - OpenAI-compatible API\")\n",
    "print(\"   - Performance: <2s for short text\")\n",
    "print(\"\")\n",
    "print(\"üí° Why This Architecture:\")\n",
    "print(\"   ‚úÖ Qwen2.5-Omni model CAN generate audio (confirmed in HuggingFace docs)\")\n",
    "print(\"   ‚ùå vLLM API does NOT expose audio output (API limitation)\")\n",
    "print(\"   ‚úÖ Therefore: Use vLLM for ASR+Q&A, gTTS for TTS\")\n",
    "print(\"   üìñ See: docs/QWEN_INVESTIGATION_FINDINGS.md\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Review benchmark results in /home/ubuntu/phase0-results/benchmarks/\")\n",
    "print(\"2. Compare against success criteria in docs/PHASE0_REPORT.md\")\n",
    "print(\"3. Verify all metrics meet Phase 0 requirements:\")\n",
    "print(\"   - ASR latency <3s for 30s audio ‚úÖ\")\n",
    "print(\"   - Q&A response <5s ‚úÖ\")\n",
    "print(\"   - TTS latency <2s for short text üéØ\")\n",
    "print(\"   - End-to-end <10s total üéØ\")\n",
    "print(\"4. Make Go/No-Go decision for Phase 1 infrastructure\")\n",
    "print(\"5. Document findings and update PHASE0_REPORT.md\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
